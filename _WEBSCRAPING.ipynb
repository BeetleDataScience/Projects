{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a406238-6841-416a-8db2-035b399183db",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "es una técnica utilizada para extraer grandes cantidades de datos de sitios web. Los datos de los sitios web no están estructurados, y el web scraping permite convertirlos en estructurados. \n",
    "** aspectos importantes **\n",
    "1. Recogida de datos: El web scraping es un método primario de recogida de datos de Internet. Estos datos pueden utilizarse para análisis, investigación, etc.\n",
    "2. Aplicaciones en tiempo real: El web scraping se utiliza para aplicaciones en tiempo real como actualizaciones meteorológicas, comparación de precios, etc.\n",
    "3. Aprendizaje automático: El web scraping proporciona los datos necesarios para entrenar modelos de aprendizaje automático\n",
    "\n",
    "Aplicaciones del Web Scraping\n",
    "1. Comparación de precios: Servicios como ParseHub utilizan el web scraping para recopilar datos de sitios web de compras en línea y utilizarlos para comparar los precios de los productos.\n",
    "\n",
    "2. Recopilación de direcciones de correo electrónico: Muchas empresas que utilizan el correo electrónico como medio de marketing, utilizan el web scraping para recopilar ID de correo electrónico y luego enviar correos electrónicos masivos.\n",
    "\n",
    "3. Social Media Scraping: El web scraping se utiliza para recopilar datos de sitios web de medios sociales como Twitter para averiguar qué es tendencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d261263-780b-4098-aa17-e34c69aba7eb",
   "metadata": {},
   "source": [
    "## BeautifulSoup \n",
    "BeautifulSoup es una librería Python que se utiliza con fines de web scraping para extraer los datos de archivos HTML y XML. Crea un árbol de análisis a partir del código fuente de la página que puede utilizarse para extraer datos de forma jerárquica y más legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3f35c4-8e5c-4f15-852a-55aa6ee3fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "#URL = \"http://www.example.com\"\n",
    "#page = requests.get(URL)\n",
    "#soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6826e-e110-4611-bd83-d26bc927eec3",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "es un framework de rastreo web colaborativo y de código abierto para Python. Se utiliza para extraer los datos de la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babae871-68a7-466f-8df4-85fdd0517eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scrapy\n",
    "#class QuotesSpider(scrapy.Spider):\n",
    "    #name = \"quotes\"\n",
    "    #start_urls = ['http://quotes.toscrape.com/tag/humor/',]\n",
    "    #def parse(self, response):\n",
    "        #for quote in response.css('div.quote'):\n",
    "            #yield {'quote': quote.css('span.text::text').get()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4609439-9f75-4058-ad6e-d8118ded60be",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "es una herramienta utilizada para controlar los navegadores web a través de programas y automatizar las tareas del navegador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b6b1bc-3073-4e7b-88ba-d826575c281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium import webdriver\n",
    "#driver = webdriver.Firefox()\n",
    "#driver.get(\"http://www.example.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714efa25-732c-46f1-bde3-5a848dcd6905",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5822f72e-0157-4e93-b4be-7716eb5acf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"mamba\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml==4.6.4\n",
      "  Using cached lxml-4.6.4.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: lxml\n",
      "  Building wheel for lxml (setup.py): started\n",
      "  Building wheel for lxml (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for lxml\n",
      "Failed to build lxml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [73 lines of output]\n",
      "  Building lxml version 4.6.4.\n",
      "  C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-6xd1es8g\\lxml_8dc831381fd84018a9edba89739e086c\\setup.py:67: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "  Building without Cython.\n",
      "  Building against pre-built libxml2 andl libxslt libraries\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\builder.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\cssselect.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\doctestcompare.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\ElementInclude.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\pyclasslookup.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\sax.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\usedoctest.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\_elementpath.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\__init__.py -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.py -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\builder.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\clean.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\defs.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\diff.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\ElementSoup.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\formfill.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\html5parser.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\soupparser.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\usedoctest.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\_diffcommand.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\_html5builder.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\_setmixin.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  copying src\\lxml\\html\\__init__.py -> build\\lib.win-amd64-cpython-312\\lxml\\html\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\n",
      "  copying src\\lxml\\isoschematron\\__init__.py -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\n",
      "  copying src\\lxml\\etree.h -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\etree_api.h -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\lxml.etree.h -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\lxml.etree_api.h -> build\\lib.win-amd64-cpython-312\\lxml\n",
      "  copying src\\lxml\\includes\\c14n.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\config.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\dtdvalid.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etreepublic.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\htmlparser.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\relaxng.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\schematron.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\tree.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\uri.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xinclude.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlerror.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlparser.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlschema.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xpath.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xslt.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etree_defs.h -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\lxml-version.h -> build\\lib.win-amd64-cpython-312\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\rng\n",
      "  copying src\\lxml\\isoschematron\\resources\\rng\\iso-schematron.rng -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\rng\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\RNG2Schtrn.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\XSD2Schtrn.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\n",
      "  creating build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_abstract_expand.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_dsdl_include.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_message.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_skeleton_for_xslt1.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_svrl_for_xslt1.xsl -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\readme.txt -> build\\lib.win-amd64-cpython-312\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for lxml\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (lxml)\n",
      "\"mamba\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests==2.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests==2.26.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests==2.26.0) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests==2.26.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests==2.26.0) (3.7)\n"
     ]
    }
   ],
   "source": [
    "# para este ejercicio requerira instalar varias librerias de python\n",
    "!mamba install bs4==4.10.0 -y\n",
    "!pip install lxml==4.6.4\n",
    "!mamba install html5lib==1.1 -y\n",
    "!pip install pandas\n",
    "!pip install requests==2.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6ae2f3-39b7-4e6c-ba57-17da4e81455c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (4.11.0)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Installing collected packages: beautifulsoup4\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.3\n",
      "    Uninstalling beautifulsoup4-4.12.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.3\n",
      "Successfully installed beautifulsoup4-4.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5d9dd5e-36ee-45fb-b29e-a854a92e9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 18.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "Successfully installed pandas-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires requests<3,>=2.27, but you have requests 2.26.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb2304e4-9b19-4d62-aec3-dabc22678c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53b61638-95c1-4b2b-8a26-e8af8897cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # este modulo ayuda al web scrapping.\n",
    "import requests  # este modulo ayuda a descargar la pagina web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57d082a8-1d41-4372-a880-f1448968943e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<title>Page Title</title>\n",
       "</head>\n",
       "<body>\n",
       "<h3><b id='boldest'>Lebron James</b></h3>\n",
       "<p> Salary: $ 92,000,000 </p>\n",
       "<h3> Stephen Curry</h3>\n",
       "<p> Salary: $85,000, 000 </p>\n",
       "<h3> Kevin Durant </h3>\n",
       "<p> Salary: $73,200, 000</p>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "<h3><b id='boldest'>Lebron James</b></h3>\n",
    "<p> Salary: $ 92,000,000 </p>\n",
    "<h3> Stephen Curry</h3>\n",
    "<p> Salary: $85,000, 000 </p>\n",
    "<h3> Kevin Durant </h3>\n",
    "<p> Salary: $73,200, 000</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9009cb15-7eff-46f4-8ed5-27956c415059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mire la cadena string del mismo HTML\n",
    "html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e80b20fd-b0c3-4329-a077-95544dd7a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para analizar un documento, páselo al constructor de BeautifulSoup, el objeto BeautifulSoup, que representa el documento como una estructura de datos anidada:\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48ef3375-887a-4427-aac0-1a508c9f4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Page Title\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h3>\n",
      "   <b id=\"boldest\">\n",
      "    Lebron James\n",
      "   </b>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $ 92,000,000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Stephen Curry\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $85,000, 000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Kevin Durant\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $73,200, 000\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Primero, el documento se convierte a Unicode, (similar a ASCII), y las entidades HTML se convierten a caracteres Unicode. Beautiful Soup transforma un documento HTML complejo en un árbol complejo de objetos Python. El objeto BeautifulSoup puede crear otros tipos de objetos \n",
    "# Podemos utilizar el método prettify() para mostrar el HTML en la estructura anidada:\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27b8a24f-9edb-4c5e-b4eb-483d5a5c3129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object: <title>Page Title</title>\n"
     ]
    }
   ],
   "source": [
    "# tags, ahora veremos una parte muy importante del web scraping\n",
    "# Digamos que queremos el título de la página y el nombre del jugador mejor pagado, podemos utilizar el objeto Tag. El objeto Tag corresponde a una etiqueta HTML del documento original, por ejemplo, la etiqueta title.\n",
    "\n",
    "tag_object=soup.title\n",
    "print(\"tag object:\",tag_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e464774f-d3fd-4959-99ac-1b996f74c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object type: <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(\"tag object type:\",type(tag_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3755aabd-be17-460f-afdc-760d83bc9864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si hay más de una etiqueta con el mismo nombre, se llama al primer elemento con ese nombre de etiqueta, que corresponde al jugador más pagado:\n",
    "tag_object=soup.h3\n",
    "tag_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dabb2fcd-c0ef-42e4-aaad-ab5ad80fb76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b id=\"boldest\">Lebron James</b>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Al ser un arbol, podemos tanto buscar una rama hija como una padre\n",
    "# la hija es\n",
    "tag_child =tag_object.b\n",
    "tag_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38809473-d33e-4c98-88d2-571ca4c364a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_tag=tag_child.parent\n",
    "parent_tag\n",
    "\n",
    "# En este caso es el mismo que el tag_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85ce2d33-1936-43c3-9e0f-fe66fc11b246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# el tag_object es el hijo del elemento del cuerpo\n",
    "tag_object.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da3917e0-6a24-44f9-a38b-81383147c708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $ 92,000,000 </p>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mientras que el hijo del tag_object es el elemento del parrafo\n",
    "sibling_1=tag_object.next_sibling\n",
    "sibling_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6719d943-c5d3-4060-91dd-0f47ed2da4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3> Stephen Curry</h3>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sibling_2 es el elemento de cabecera que también es hermano tanto de sibling_1 como de tag_object\n",
    "sibling_2=sibling_1.next_sibling\n",
    "sibling_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "caa951c5-dc78-4a6d-b83c-cfbde1128e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si el tag tiene atributos el 'id' = 'boldest'. y puedes acceder a sus atributos tratandola como un diccionario\n",
    "tag_child['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c87c15f-dfd8-4581-9061-2b6352285d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'boldest'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acceda a su tag como un diccionario con el comando attrs \n",
    "tag_child.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "76473940-c8f1-43d1-a832-32022f126d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tambien puede acceder a los atributos de su tag con la python .get()\n",
    "tag_child.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a17c803-3d5a-494f-b3d2-e370680c7de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nabigable String\n",
    "# Una cadena corresponde a un trozo de texto o contenido dentro de una etiqueta. Beautiful Soup utiliza la clase NavigableString para contener este texto. En nuestro HTML podemos obtener el nombre del primer jugador extrayendo el sting del objeto Tag tag_child de la siguiente manera\n",
    "tag_string=tag_child.string\n",
    "tag_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "142ea19f-8379-4655-8423-4d0f3ae72517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lo verificamos \n",
    "type(tag_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a4ea819-7bc3-4fa3-b5de-7d50383ce12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los NavigableString es como una cadena de string de python solo lo diferencia que tambien soporta algunas caracteristicas de Beatiful Soup\n",
    "# pero tambien lo podemos convertir a str \n",
    "unicode_string= str(tag_string)\n",
    "unicode_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e837e-77b1-4359-9f54-a85c85204419",
   "metadata": {},
   "source": [
    "### Filter \n",
    "\n",
    "Los filtros permiten encontrar patrones complejos, el filtro más simple es un string. En esta sección pasaremos una string a un método de filtro diferente y Beautiful Soup realizará una coincidencia contra esa cadena exacta.  Considera el siguiente HTML de lanzamientos de cohetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78857f02-a0a6-45c7-b1d6-d31d7196120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <td id='flight' >Flight No</td>\n",
       "    <td>Launch site</td> \n",
       "    <td>Payload mass</td>\n",
       "   </tr>\n",
       "  <tr> \n",
       "    <td>1</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
       "    <td>300 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>2</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
       "    <td>94 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>3</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n",
       "    <td>80 kg</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table>\n",
    "  <tr>\n",
    "    <td id='flight' >Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "   </tr>\n",
    "  <tr> \n",
    "    <td>1</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e318a571-6124-437e-b98d-618a5fbaba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Su respectivo string\n",
    "table=\"<table><tr><td id='flight' >Flight No</td><td>Launch site</td><td>Payload mass</td></tr><tr><td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td><td>80 kg</td></tr></table>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd1479a4-477d-40a4-a1c3-946055065dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y lo pasamos por BeautifulSoup \n",
    "table_bs = BeautifulSoup(table, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81999381-7326-4cc9-9ecc-50f738d549b3",
   "metadata": {},
   "source": [
    "### Find All \n",
    "El método find_all() busca entre los descendientes de una etiqueta y recupera todos los descendientes que coincidan con sus filtros.\n",
    "\n",
    "Firma del método find_all(name, attrs, recursive, string, limit, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0519f852-f641-4228-a81d-e197156d71c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>,\n",
       " <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mire como se extran todos los titulos utilizando solo el nombre tag ('tr')\n",
    "table_rows=table_bs.find_all('tr')\n",
    "table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8cb5e114-056b-48ad-9c28-50e4bb1c7a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos permite seleccionar para ('tr') que buscamos como si fuera un diccionario\n",
    "first_row =table_rows[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "250483c5-d3c2-44b5-a6d0-c406a949515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "# su tipo es Tag\n",
    "print(type(first_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "48b64387-3e30-446e-a186-9abbb7134cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td id=\"flight\">Flight No</td>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#buscamos su rama hija\n",
    "first_row.td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "25ae7da6-3947-4bd0-a513-043e81832e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 is <tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>\n",
      "row 1 is <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>\n",
      "row 2 is <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>\n",
      "row 3 is <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>\n"
     ]
    }
   ],
   "source": [
    "# Si realizamos un bucle, podremos observar como cada fila de la tabla corresponde a un elemento\n",
    "for i,row in enumerate(table_rows):\n",
    "    print(\"row\",i,\"is\",row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b650e63-9aa3-4350-a6b1-7d948e3e0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0\n",
      "colunm 0 cell <td id=\"flight\">Flight No</td>\n",
      "colunm 1 cell <td>Launch site</td>\n",
      "colunm 2 cell <td>Payload mass</td>\n",
      "row 1\n",
      "colunm 0 cell <td>1</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td>\n",
      "colunm 2 cell <td>300 kg</td>\n",
      "row 2\n",
      "colunm 0 cell <td>2</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>\n",
      "colunm 2 cell <td>94 kg</td>\n",
      "row 3\n",
      "colunm 0 cell <td>3</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td>\n",
      "colunm 2 cell <td>80 kg</td>\n"
     ]
    }
   ],
   "source": [
    "# Como cada fila es un objeto cell, podemos aplicarle el método find_all y extraer las celdas de la tabla en las celdas del objeto usando el tag td, esto es todos los hijos con el nombre td. El resultado es una lista, cada elemento corresponde a una celda y es un objeto Tag, podemos iterar a través de esta lista también. Podemos extraer el contenido utilizando el atributo string.\n",
    "for i,row in enumerate(table_rows):\n",
    "    print(\"row\",i)\n",
    "    cells=row.find_all('td')\n",
    "    for j,cell in enumerate(cells):\n",
    "        print('colunm',j,\"cell\",cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8fc921d2-4af3-47a1-9e97-b447eae34786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>,\n",
       " <td id=\"flight\">Flight No</td>,\n",
       " <td>Launch site</td>,\n",
       " <td>Payload mass</td>,\n",
       " <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>,\n",
       " <td>1</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td>,\n",
       " <td>300 kg</td>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <td>2</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n",
       " <td>94 kg</td>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>,\n",
       " <td>3</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td>,\n",
       " <td>80 kg</td>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Si utilizamos una lista, podemos compararla con cualquier elemento de esa lista.\n",
    "list_input=table_bs .find_all(name=[\"tr\", \"td\"])\n",
    "list_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "897c3365-d7ef-4c22-989d-f03ff2674903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td id=\"flight\">Flight No</td>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all(id=\"flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "856224fb-c4ae-4845-b7ac-fbdfbd8b6e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\n",
    "list_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ae74dad7-0d7a-486f-97a7-7a9200caad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619fea8-4cf6-460b-9b10-1df9d5822cab",
   "metadata": {},
   "source": [
    "### Sring \n",
    "Tambien se puede buscar string en lugar de etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6668854-edc0-428f-8627-f43d9defd1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Florida', 'Florida']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all(string=\"Florida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92a50a5c-9ada-484e-a861-567aaebbc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otro ejemplo, \n",
    "# observe como son 2 tablas, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a7155b59-fb77-44f1-8334-a6b3a952c89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Rocket Launch </h3>\n",
       "\n",
       "<p>\n",
       "<table class='rocket'>\n",
       "  <tr>\n",
       "    <td>Flight No</td>\n",
       "    <td>Launch site</td> \n",
       "    <td>Payload mass</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>1</td>\n",
       "    <td>Florida</td>\n",
       "    <td>300 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>2</td>\n",
       "    <td>Texas</td>\n",
       "    <td>94 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>3</td>\n",
       "    <td>Florida </td>\n",
       "    <td>80 kg</td>\n",
       "  </tr>\n",
       "</table>\n",
       "</p>\n",
       "<p>\n",
       "\n",
       "<h3>Pizza Party  </h3>\n",
       "  \n",
       "    \n",
       "<table class='pizza'>\n",
       "  <tr>\n",
       "    <td>Pizza Place</td>\n",
       "    <td>Orders</td> \n",
       "    <td>Slices </td>\n",
       "   </tr>\n",
       "  <tr>\n",
       "    <td>Domino's Pizza</td>\n",
       "    <td>10</td>\n",
       "    <td>100</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Little Caesars</td>\n",
       "    <td>12</td>\n",
       "    <td >144 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Papa John's </td>\n",
       "    <td>15 </td>\n",
       "    <td>165</td>\n",
       "  </tr>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3>Rocket Launch </h3>\n",
    "\n",
    "<p>\n",
    "<table class='rocket'>\n",
    "  <tr>\n",
    "    <td>Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Florida</td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Texas</td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Florida </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "<h3>Pizza Party  </h3>\n",
    "  \n",
    "    \n",
    "<table class='pizza'>\n",
    "  <tr>\n",
    "    <td>Pizza Place</td>\n",
    "    <td>Orders</td> \n",
    "    <td>Slices </td>\n",
    "   </tr>\n",
    "  <tr>\n",
    "    <td>Domino's Pizza</td>\n",
    "    <td>10</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Little Caesars</td>\n",
    "    <td>12</td>\n",
    "    <td >144 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Papa John's </td>\n",
    "    <td>15 </td>\n",
    "    <td>165</td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2cb434bd-2efd-4455-8cb7-3f20b36ff006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el string de las tablas\n",
    "two_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3f0d36d2-62fa-424a-be23-ba274ca23198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el BeautifulSoup\n",
    "two_tables_bs= BeautifulSoup(two_tables, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7b81957d-15d5-4708-b45c-23a4097075dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Buscamos la primera table usando el tag name table\n",
    "two_tables_bs.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "605c6c88-2e08-4381-900a-96d2539ec1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos filtrar por el atributo class para encontrar la segunda tabla, pero como class es una palabra clave en Python, añadimos un guión bajo.\n",
    "two_tables_bs.find(\"table\",class_='pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a9f51-da2e-4d0f-8134-3fefdc432f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
